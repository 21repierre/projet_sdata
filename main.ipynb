{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bc2a2e1e4af677e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Projet SDATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826a2aa11dd47751",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercice 1\n",
    "\n",
    "### Analyse des données \n",
    "\n",
    "La première étape est d'analyser les données contenues dans les fichiers ***spotify_dataset_train.csv*** et ***spotify_dataset_test.csv***. \n",
    "Avant de commencer, il nous faut formatter les données afin qu'elles soient numériques. Cela permettra ensuite d'utiliser certaines métriques comme les matrices de confusion par exemple.\n",
    "\n",
    "Dans le code ci-dessous, nous chargeons donc les données d'entraînement et de test dans `dataset_train` et `dataset_test` respectivement.\n",
    "\n",
    "Ensuite, le première variable à formatter est la variable ***explicit*** qui est, par défaut, représentée par des booléens. On transforme alors les booléens en 0 et 1 pour obtenir une valeur numérique.\n",
    "\n",
    "La deuxième variable que nous devons modifier est la ***release_date***. Celle-ci est représentée par un `datetime` qui est un format de représentation des dates. Nous voulons obtenir un `timestamp` représentant la date sous la forme d'un nombre. \n",
    "Nous repérons plusieurs problèmes:\n",
    "* Les données ne sont pas toutes au même format: par exemple: `2015-11-06` ou `1997`\n",
    "* Certaines musiques sont sortis avant le 1er janvier 1970, référence du temps Unix\n",
    "\n",
    "Pour résoudre ce problème, nous parcourons les ***release_dates***, on détecte le format, puis on le remplace par un `timestamp` en utilisant la time zone 'UTC' (Coordinated Universal Time) permettant d'éviter les soucis avant pour les dates avant 1970. Afin d'éviter les problèmes, nous affichons les valeurs que nous avons pas réussi à transformer correctement afin de pouvoir adapter le code si nécessaire.\n",
    "\n",
    "Enfin, nous profitons de la boucle *for* pour remplacer le nom des genres en l'index associé dans la liste des genres, permettant ainsi d'obtenir un dataset complet ne contenant que des valeurs numériques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "dataset_train = pandas.read_csv(\"dataset/spotify_dataset_train.csv\")\n",
    "dataset_test = pandas.read_csv(\"dataset/spotify_dataset_test.csv\")\n",
    "\n",
    "GENRE_MAP = ['blues', 'chanson', 'classical', 'country', 'dance', 'disco', 'edm', 'electro', 'folk', 'hip hop', 'jazz',\n",
    "             'latin', 'metal', 'pop', 'punk', 'r&b', 'rap', 'reggae', 'rock', 'salsa', 'soul', 'techno']\n",
    "\n",
    "dataset_train['explicit'] = dataset_train['explicit'].astype(np.int8)\n",
    "dataset_test['explicit'] = dataset_test['explicit'].astype(np.int8)\n",
    "\n",
    "for i, date in enumerate(dataset_train['release_date']):\n",
    "    # Detect the date format and replace it with a utc timestamp for the train dataset\n",
    "    if len(date.split('-')) == 3:\n",
    "        dataset_train.loc[i, 'release_date'] = datetime.datetime.strptime(date, \"%Y-%m-%d\").replace(\n",
    "            tzinfo=datetime.timezone.utc).timestamp()\n",
    "    elif len(date.split('-')) == 2:\n",
    "        dataset_train.loc[i, 'release_date'] = datetime.datetime.strptime(date, \"%Y-%m\").replace(\n",
    "            tzinfo=datetime.timezone.utc).timestamp()\n",
    "    elif len(date.split('-')) == 1:\n",
    "        dataset_train.loc[i, 'release_date'] = datetime.datetime.strptime(date, \"%Y\").replace(\n",
    "            tzinfo=datetime.timezone.utc).timestamp()\n",
    "    else:\n",
    "        print(date)\n",
    "\n",
    "    # Transform the genre string by the index equivalent\n",
    "    dataset_train.loc[i, 'genre'] = GENRE_MAP.index(dataset_train.loc[i, 'genre'])\n",
    "\n",
    "for i, date in enumerate(dataset_test['release_date']):\n",
    "    # Detect the date format and replace it with a utc timestamp for the test dataset\n",
    "    if len(date.split('-')) == 3:\n",
    "        dataset_test.loc[i, 'release_date'] = datetime.datetime.strptime(date, \"%Y-%m-%d\").replace(\n",
    "            tzinfo=datetime.timezone.utc).timestamp()\n",
    "    elif len(date.split('-')) == 2:\n",
    "        dataset_test.loc[i, 'release_date'] = datetime.datetime.strptime(date, \"%Y-%m\").replace(\n",
    "            tzinfo=datetime.timezone.utc).timestamp()\n",
    "    elif len(date.split('-')) == 1:\n",
    "        dataset_test.loc[i, 'release_date'] = datetime.datetime.strptime(date, \"%Y\").replace(\n",
    "            tzinfo=datetime.timezone.utc).timestamp()\n",
    "    else:\n",
    "        print(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544ca3900b340f9a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Il est maintenant temps de visualiser les données et de les comparer. \n",
    "\n",
    "La première chose que nous faisons est une matrice des corrélations, en utilisant `seaborn` et `matplotlib` pour les affichages.\n",
    "On remarque grâce à cette matrice que peu de données sont réellement corrélées. Les seules choses que nous pouvons relever sont:\n",
    "* ***loudness*** positivement corrélée avec ***energy***\n",
    "* ***speechiness*** est positivement corrélée avec ***explicit***\n",
    "* ***accousticness*** est négativement corrélée avec ***loudness*** et ***energy***\n",
    "\n",
    "Ces corrélations sont tout à fait naturelles et semblent cohérentes avec le sujet. Par exemple, il n'est pas surprenant que les musiques contenant beaucoup de paroles aient une plus grande probabilité d'être explicites, ou encore il n'est pas surpenant que les musiques accoustiques soient peu énergiques.\n",
    "\n",
    "La deuxième visualisation que nous faisons est un simple diagramme en barres des différentes classes représentées. Cela nous montre une chose intéressante: les différents genres ne contiennent pas du tout la même quantité de données, ce qui signifie, au vu des résultats, que les données choisies sont principalement basées sur la popularité des différents genres plutôt que dans l'objectif de faire un apprentissage équivalent sur chaque genre.\n",
    "\n",
    "À l'aide de ces rapides analyses, nous supposons qu'il ne sera pas évident de prédire les classes car, premièrement, au vu de la matrice des corrélations, le genre n'est corrélé avec aucune autre variable, de plus, comme certains genres ne sont pas très présent, il y aura peu de données pour entraîner notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68319fe79de4c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Correlation\n",
    "corr = dataset_train.corr()\n",
    "plt.figure(figsize=(15, 10))\n",
    "seaborn.heatmap(corr,\n",
    "                cmap=seaborn.diverging_palette(230, 20, as_cmap=True),\n",
    "                vmin=-1.0, vmax=1.0,\n",
    "                square=False, annot=True)\n",
    "\n",
    "# Genre count\n",
    "plt.figure(figsize=(20, 10))\n",
    "genre_counts = [np.count_nonzero(dataset_train['genre'] == x) for x in range(len(GENRE_MAP))]\n",
    "classes_weight = {}\n",
    "for i in range(len(GENRE_MAP)):\n",
    "    classes_weight[i] = genre_counts[i]\n",
    "plt.bar(GENRE_MAP, genre_counts, width=1, align='center')\n",
    "plt.title('Genre count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34570202ef6f16a8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Nous décidons ensuite d'ajouter une couche d'analyse plus spécifique. Nous avons 2 types de variables: \n",
    "* Les variables allant d'un minimum à un maximum: exemple ***loudness***, plus elle est grande plus la musique est forte\n",
    "* Les variables représentant une caractéristiques: exemple ***key*** représentant la note principale du morceau\n",
    "\n",
    "Nous faisons donc une analyse pour ces deux catégories afin de découvrir quels genres se cachent derrière les différentes variables. \n",
    "\n",
    "Dans ce premier programme, nous gardons les paramètres du type min-max qui sont intéressants et comparons les moyennes sur chaque genre. La seule variable qui n'est pas du type min-max que nous utilisons malgré tout est la variable ***explicit***, cependant, contrainement à un paramètre comme ***mode*** qui vaut 0 pour majeur et 1 pour mineur, il est intéressant de faire une moyenne du paramètre ***explicit*** (valant 0 pour du contenu non explicite et 1 sinon) puisqu'il nous donnera un pourcentage de musiques contenant du contenu explicite au sein d'un genre.\n",
    "\n",
    "Pour ce faire, nous utilisons `numpy.mean` qui permet de faire la moyenne sur chaque variable, le tout en construisant une sous-liste ne contenant que les données d'un genre donné à l'aide de `numpy.where`. Cela nous crée donc, pour chaque genre, une liste de taille *nombre_de_variables_intéressantes* contenant, pour chaque variable, la moyenne de cette variable sur ce genre. \n",
    "\n",
    "Grâce à cela, nous pouvons rentrer les moyennes sur chaque variable pour chaque genre dans un tableau puis simplement prendre le minimum et maximum pour chaque variable à l'aide de `numpy.min` et `numpy.max`.\n",
    "\n",
    "Une fois encore, les résultats ci-dessous (voir la réponse du code) semblent cohérents. Par exemple nous apprenons que le genre le plus populaire est le r&b et le moins populaire est la salsa, ou encore le genre le moins acoustique est la techno et le plus acoustique est la musique classique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae86320812e6b165",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Keep only the interesting features to compare\n",
    "interesting_features = np.array(\n",
    "    [\"explicit\", \"popularity\", \"danceability\", \"energy\", \"loudness\", \"speechiness\", \"acousticness\", \"instrumentalness\",\n",
    "     \"valence\", \"tempo\", \"duration_ms\"])\n",
    "genres_compared = np.zeros(shape=(interesting_features.size, len(GENRE_MAP)))\n",
    "\n",
    "# Fill the array column by column with the mean of each feature for each genre\n",
    "for genre in range(genres_compared.shape[1]):\n",
    "    # Returns an array containing the means for every feature\n",
    "    genre_means = np.mean(dataset_train.iloc[np.where(dataset_train['genre'] == genre)], axis=0)\n",
    "    for feature in range(genres_compared.shape[0]):\n",
    "        genres_compared[feature][genre] = genre_means[interesting_features[feature]]\n",
    "\n",
    "# Display the genre for the min and max value for each interesting feature\n",
    "print(f\"{'Feature':<20} {'Min':<15} {'Value':<15} {'Max':<15} {'Value':<15}\")\n",
    "for feature in range(genres_compared.shape[0]):\n",
    "    # Find the index of the max and the min to get the genre associated\n",
    "    min_genre = list(genres_compared[feature]).index(np.min(genres_compared[feature]))\n",
    "    max_genre = list(genres_compared[feature]).index(np.max(genres_compared[feature]))\n",
    "\n",
    "    print(f\"{interesting_features[feature]:<20} \", end=\"\")\n",
    "    print(f\"{GENRE_MAP[min_genre]:<15} \", end=\"\")\n",
    "    print(f\"{round(genres_compared[feature][min_genre], 3):<15} \", end=\"\")\n",
    "    print(f\"{GENRE_MAP[max_genre]:<15} \", end=\"\")\n",
    "    print(f\"{round(genres_compared[feature][max_genre], 3):<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb7648c906cd53e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Dans ce deuxième programme, nous regardons donc le deuxième type de variable et cherchons à trouver la valeur la plus et la moins populaire parmi tous les genres. Les deux variables intéressantes sont ***key*** et ***mode*** représentant respectivement la note principale du morceau, et le caractère majeur ou mineur de la tonalité.\n",
    "\n",
    "De plus, avec ces deux paramètres, nous pouvons construire un dictionnaire des tonalités et trouver les tonalités les plus et moins populaires. L'intérêt de faire cela est que ce n'est pas parce que la note la plus populaire est le Ré et que le mode le plus populaire est le mineur que nécessairement la tonalité la plus populaire sera le Ré mineur !\n",
    "\n",
    "Pour faire cela, nous contruisons trois dictionnaires contenant les différentes notes, modes et tonalités, en mettant leur compte à 0. Afin de construire la clé du dictionnaire des tonalités et faciliter les affichages, nous rajoutons également les équivalent en français des indices des dictionnaires des notes et des modes.\n",
    "> Note au lecteur: certaines tonalités présentées ici n'existent pas en musique, comme par exemple le Sol# majeur, qui en réalité est du La*b* majeur. \n",
    "> Cependant, afin de faciliter la reconstitution des clés et la lecture du code, nous avons décider de n'utiliser que les dièses.\n",
    "\n",
    "Ensuite, il suffit de parcourir les données et de compter les occurences de chaque clé, puis de trouver le maximum et minimum dans chaque dictionnaire afin de trouver les plus et moins populaires.\n",
    "\n",
    "Nous avons donc les résultats suivant (voir la réponse du programme): le Sol est la note la plus populaire, le majeur est plus populaire que le mineur, et la tonalité la plus utilisée est le Do Majeur ! À l'inverse, la note la moins populaire est le Do# et la tonalité la moins utilisée est le Do# mineur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779a2f7e86ba5871",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definition of the different dictionnaries associated\n",
    "keys = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0}\n",
    "keys_equivalent = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n",
    "\n",
    "modes = {0: 0, 1: 0}\n",
    "modes_equivalent = [\"Minor\", \"Major\"]\n",
    "\n",
    "tonalities = {\"C_Major\": 0,\n",
    "              \"C#_Major\": 0,\n",
    "              \"D_Major\": 0,\n",
    "              \"D#_Major\": 0,\n",
    "              \"E_Major\": 0,\n",
    "              \"F_Major\": 0,\n",
    "              \"F#_Major\": 0,\n",
    "              \"G_Major\": 0,\n",
    "              \"G#_Major\": 0,\n",
    "              \"A_Major\": 0,\n",
    "              \"A#_Major\": 0,\n",
    "              \"B_Major\": 0,\n",
    "              \"C_Minor\": 0,\n",
    "              \"C#_Minor\": 0,\n",
    "              \"D_Minor\": 0,\n",
    "              \"D#_Minor\": 0,\n",
    "              \"E_Minor\": 0,\n",
    "              \"F_Minor\": 0,\n",
    "              \"F#_Minor\": 0,\n",
    "              \"G_Minor\": 0,\n",
    "              \"G#_Minor\": 0,\n",
    "              \"A_Minor\": 0,\n",
    "              \"A#_Minor\": 0,\n",
    "              \"B_Minor\": 0}\n",
    "\n",
    "# Count the number each key, mode and tonality\n",
    "for i in range(len(dataset_train[\"key\"])):\n",
    "    current_key = dataset_train[\"key\"][i]\n",
    "    current_mode = dataset_train[\"mode\"][i]\n",
    "    keys[current_key] += 1\n",
    "    modes[current_mode] += 1\n",
    "    tonalities[f\"{keys_equivalent[current_key]}_{modes_equivalent[current_mode]}\"] += 1\n",
    "# Find the most used key, mode and tonality then display them\n",
    "most_used_key = max(keys.values())\n",
    "most_used_key_index = max(keys, key=keys.get)\n",
    "most_used_mode = max(modes.values())\n",
    "most_used_mode_index = max(modes, key=modes.get)\n",
    "most_used_tonality = max(tonalities.values())\n",
    "most_used_tonality_name = max(tonalities, key=tonalities.get)\n",
    "\n",
    "# Find the least used key, mode and tonality then display them\n",
    "least_used_key = min(keys.values())\n",
    "least_used_key_index = min(keys, key=keys.get)\n",
    "least_used_mode = min(modes.values())\n",
    "least_used_mode_index = min(modes, key=modes.get)\n",
    "least_used_tonality = min(tonalities.values())\n",
    "least_used_tonality_name = min(tonalities, key=tonalities.get)\n",
    "\n",
    "print(f\"The most used key is {keys_equivalent[most_used_key_index]} with a number of {most_used_key} songs\")\n",
    "print(f\"The most used mode is {modes_equivalent[most_used_mode_index]} with a number of {most_used_mode} songs\")\n",
    "print(f\"The most used tonality is {most_used_tonality_name} with a number of {most_used_tonality} songs\")\n",
    "print(\"==========================================\")\n",
    "print(f\"The least used key is {keys_equivalent[least_used_key_index]} with a number of {least_used_key} songs\")\n",
    "print(f\"The least used mode is {modes_equivalent[least_used_mode_index]} with a number of {least_used_mode} songs\")\n",
    "print(f\"The least used tonality is {least_used_tonality_name} with a number of {least_used_tonality} songs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2299da",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Une dernière analyse des données que nous faisons est un *PCA* (Principal Components Analysis).\n",
    "\n",
    "Nous entraînons donc un *PCA* de façon standard en gardant les paramètres par défaut puis affichons les courbes des variances et variances cumulées. En observant la courbe de droite, on comprends alors que nous pouvons garder 80% des informations en ne gardant que 9 composantes, ce qui pourrait réduire assez fortement la complexité des données à étudier. Cependant, étant donné qu'il n'y a pas une grande différence entre garder 9 ou 11 paramètres, nous décidons de rester avec l'intégralité des données.\n",
    "\n",
    "Enfin, nous faisons un deuxième *PCA* en projetant sur 3 dimensions afin de projeter les données en *3D* en utilisant `matplotlib`. En regardant le résultat, on peut distinguer légèrement deux zones mais il reste néanmoins difficile de tirer des conclusions. Au vu des résultats obtenus, nous en concluons sur le fait que le *PCA* n'est pas très adapté à cette situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e410be3638ba6d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create PCA and train it\n",
    "pca = PCA()\n",
    "X = dataset_train.iloc[:, dataset_train.columns != 'genre'].to_numpy()\n",
    "Y = dataset_train.iloc[:, dataset_train.columns == 'genre'].to_numpy().flatten().astype(np.int8)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "pca.fit(X)\n",
    "var_ratios = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Display the curves of the explained variance and the cumulated variance\n",
    "fig = plt.figure(figsize=(20, 5))\n",
    "\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.plot(pca.explained_variance_)\n",
    "\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.plot(var_ratios)\n",
    "\n",
    "# Scatter the data reduced to 3 dimensions\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = plt.axes(projection='3d')\n",
    "pca = PCA(n_components=3)\n",
    "X_down = pca.fit_transform(X)\n",
    "\n",
    "ax.scatter(X_down[:, 0], X_down[:, 1], X_down[:, 2], c=Y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1005d42b380e21a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Classification\n",
    "\n",
    "Il est désormais temps de passer à la classification des données en entraînant divers modèles afin de comparer leur performances. Le but est de trouver le meilleur score *F1 micro* possible, autrement dit l'*accuracy*, nous allons donc comparer les différents modèles sur ce critère et ne garder que le meilleur afin de prédire le genre des données contenues dans `dataset_test`.\n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "Le premier modèle de classification que nous essayons est un ***RandomForest***. Le modèle d'entraînement est tout à fait classique: nous séparons les données en données d'entraînement et de test que l'on utilise afin d'entraîner le modèle. Puis, nous utilisons la *cross validation* afin de déterminer la précision du modèle. Après avoir essayé différents paramètres, nous avons remarqué que les résultats étaient globalement meilleurs avec les paramètres par défaut, nous les avons donc laissé ainsi.\n",
    "\n",
    "> Note: nous n'entraînons nos modèle que sur les données contenues dans `dataset_train`. Attention à ne pas confondre `X_test` et `Y_test` avec le `dataset_test`, qui sont obtenus à l'aide de la fonction `train_test_split` sur `dataset_train`.\n",
    "\n",
    "Nous obtenons un score autour de *0.46* pour notre *F1* score, ce qui ne semble pas très haut à première vue, mais qui à la fois n'est pas complètement surprenant au vu de la difficulté de prédiction supposée plus tôt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c374561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Split the data in train and test\n",
    "X_train = dataset_train.iloc[:, dataset_train.columns != 'genre'].to_numpy()\n",
    "Y_train = dataset_train.iloc[:, dataset_train.columns == 'genre'].to_numpy().flatten().astype(np.int8)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.1)\n",
    "\n",
    "# Initialise the Random Forest and calculate the F1 score\n",
    "rf = RandomForestClassifier(criterion='gini', max_features=0.5, class_weight='balanced')\n",
    "\n",
    "scores = cross_val_score(rf, X_train, Y_train, cv=5, scoring='f1_micro')\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3ad573",
   "metadata": {},
   "source": [
    "Nous faisons ensuite une prédiction des genres des données de `dataset_test` à l'aide de notre ***RandomForest***, puis nous écrivons les résultat dans le fichier *.csv* correspondant en rajoutant une colonne.\n",
    "\n",
    "Nous ne savons pas encore à ce stade si le ***RandomForest*** sera notre meilleur modèle ou non mais le code sera réutilisable dans tous les cas si nécessaire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764dba478857df0e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the Random Forest and predict the genre of the dataset_test\n",
    "rf.fit(X_train, Y_train)\n",
    "test_pred = rf.predict(dataset_test.iloc[:].to_numpy())\n",
    "\n",
    "for i in range(len(dataset_test)):\n",
    "    dataset_test.loc[i, 'genre'] = GENRE_MAP[test_pred[i]]\n",
    "\n",
    "dataset_test.to_csv('./dataset/spotify_dataset_test_pred.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d63a1",
   "metadata": {},
   "source": [
    ">Note: après avoir fait un ***RandomForest***, nous avons également expérimenté les ***SVM*** (Support Vector Machines), mais les performances étaient fortement plus basses que pour le ***RandomForest*** et que pour les autres modèles que nous testons ci-dessous. De plus, l'implémentation du code était très similaire, c'est pourquoi nous avons décider de le retirer du NoteBook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba1c16333a62b74",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Réseau de Neuronnes\n",
    "\n",
    "Le second type de classification que nous décidons d'explorer sont les réseaux de neuronnes. Nous décidons pour cela de l'implémenter à l'aide de `pytorch`, bibliothèque très puissante en matière de Data Science en Python. Nous aurions pu utiliser `keras` comme vu en cours, mais étant plus à l'aise sur `pytorch` nous décidons d'utiliser cette bibliothèque plutôt.\n",
    "\n",
    "Ici, nous commençons par créer une classe standard de réseaux de neuronnes à l'aide de `pytorch`. Également, afin de potentiellement améliorer notre puissance de calcul, nous décidons d'utiliser `cuda`, permettant de faire tourner les calculs sur la carte graphique au lieu d'utiliser le processeur. Cela peut être un gain de performances considérable selon les caractéristiques de l'ordinateur utilisé, et ayant tous les deux des ordinateurs possédant de bonnes cartes graphiques, ce choix semblait pertinent, d'autant plus que nous prevoyons de lancer les tests en parrallèles, rendant ce choix d'autant plus intéressant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa783b16e5b29f1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# First class of a standard NN model using pytorch\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "\n",
    "    # Initialisation of the NN\n",
    "    def __init__(self, n1=20, n2=20, n3=20, n4=20, n5=20, p1=0, p2=0, p3=0, p4=0, p5=0):\n",
    "        super().__init__()\n",
    "        self.stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(16, n1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=p1),\n",
    "\n",
    "            torch.nn.Linear(n1, n2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=p2),\n",
    "\n",
    "            torch.nn.Linear(n2, n3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=p3),\n",
    "\n",
    "            torch.nn.Linear(n3, n4),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=p4),\n",
    "\n",
    "            torch.nn.Linear(n4, n5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=p5),\n",
    "\n",
    "            torch.nn.Linear(n5, 22),\n",
    "        )\n",
    "\n",
    "    # Used to predict\n",
    "    def forward(self, x):\n",
    "        logits = self.stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Detect if we're using cuda or not, meaning if we run the program on the cpu or gpu\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364f8dbd",
   "metadata": {},
   "source": [
    "Dans le code suivant, nous créons un autre type de réseau de neuronnes en utilisant des *layers* différents par rapport à précédemment permettant d'obtenir des résultats différents selon le modèle. \n",
    "\n",
    "Notre but avec cela est d'entraîner simultanément plusieurs types de réseaux de neuronnes en faisant varier les caractéristiques de chaque type et de comparer tous les modèles entre eux. Cela nous permettrait alors de trouver le meilleur type de réseau avec les meilleurs paramètres afin d'obtenir les meilleurs résultats lors de l'étape de prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71b895719c70969",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# A different model of NN using different layers\n",
    "class MusicGenreClassifier(torch.nn.Module):\n",
    "\n",
    "    # Initialisation of the NN\n",
    "    def __init__(self, n1=20, n2=20, n3=20, n4=20, p1=0, p2=0, p3=0, p4=0):\n",
    "        super(MusicGenreClassifier, self).__init__()\n",
    "\n",
    "        # Feature extraction layers\n",
    "        self.feature_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(16, n1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(n1),\n",
    "            torch.nn.Dropout(p=p1),\n",
    "\n",
    "            torch.nn.Linear(n1, n2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(n2),\n",
    "            torch.nn.Dropout(p=p2),\n",
    "\n",
    "            torch.nn.Linear(n2, n3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(n3),\n",
    "            torch.nn.Dropout(p=p3)\n",
    "        )\n",
    "        self.lstm_layers = torch.nn.LSTM(\n",
    "            input_size=int(n3),\n",
    "            hidden_size=int(n4),\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=int(p4),\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Classification layers\n",
    "        self.classification_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2 * n4, n4),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(n4),\n",
    "            torch.nn.Dropout(p=p4),\n",
    "\n",
    "            torch.nn.Linear(n4, 22)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through feature extraction layers\n",
    "        x = self.feature_layers(x)\n",
    "        lstm_out, _ = self.lstm_layers(x)\n",
    "        x = self.classification_layers(lstm_out)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce18ccf",
   "metadata": {},
   "source": [
    "Ensuite, afin de pouvoir utiliser les modèles de `pytorch` créés précedemment, il nous faut transformer les données dans un format compatible avec `pytorch`. Le format voulu est le format de ***DataLoader*** qui est créé à l'aide des ***tensors*** de `pytorch`.\n",
    "\n",
    "C'est donc ce que nous faisons ici en séparant une fois de plus le jeu de données en donées de test et d'entraînement, puis en les transformant en ***DataLoader*** pour les renvoyer à `pytorch`. Nous utilisons également le paramètre `batch_size`, valant 32 par défaut, afin d'augmenter la possibilité de personnalisation si nous le souhaitons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffde185a15ec0a0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "# Load the data in a pytorch compatible format using tensors\n",
    "def load_data_genre(batch_size=32):\n",
    "    # Split the data\n",
    "    X = dataset_train.iloc[:, dataset_train.columns != 'genre'].to_numpy()\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    Y = dataset_train.iloc[:, dataset_train.columns == 'genre'].to_numpy().flatten().astype(np.float64)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=12345)\n",
    "\n",
    "    # Creates the DataLoaders using tensors\n",
    "    TX_train = torch.tensor(X_train, dtype=torch.float).to(device)\n",
    "    Ty_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "\n",
    "    TX_test = torch.tensor(X_test, dtype=torch.float).to(device)\n",
    "    Ty_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "    t_dataset_train = TensorDataset(TX_train, Ty_train)\n",
    "    t_dataset_test = TensorDataset(TX_test, Ty_test)\n",
    "\n",
    "    train_batch = DataLoader(t_dataset_train, batch_size=batch_size, shuffle=True)\n",
    "    validation_batch = DataLoader(t_dataset_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_batch, validation_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69cc67b",
   "metadata": {},
   "source": [
    "Il est enfin temps de créer le processus d'entraînement des réseaux de neuronnes.\n",
    "\n",
    "On commence donc d'abord par récupérer les caractéristiques de la configuration donnée en paramètre et on crée notre modèle. On initialise également un *optimizer Adam*, permettant d'optimiser l'apprentissage au mieux. \n",
    "\n",
    "Par ailleurs, nous avons ici la possibilité d'ajouter des *checkpoints*, ce qui peut être très utile lors de l'apprentissage. Cela permet de stopper l'apprentissage à un endroit donné, puis de le reprendre par la suite en repartant du *checkpoint* configuré précédemment. Nous utilisons également le module `tqdm`, très pratique pour l'affichage dans le terminal de l'avancement d'un programme au fur et à mesure de son déroulement.\n",
    "\n",
    "> Bien qu'il soit très pratique, le module `tqdm` peut parfois impacter sévèrement les performances d'un programme, c'est notamment pour cela que nous décidons de le mettre en tant que paramètre optionnel.\n",
    "\n",
    "Ensuite, pour chaque *epoch*, nous entraînons notre réseau de neuronnes en nous faisant aider de l'*optimizer*, puis nous faisons les tests en calculant au fur et à mesure les pertes et la précision du modèle. La dernière partie du programme n'est qu'un petit bout de code permettant l'ajout d'un checkpoint à un endroit donné si cela était souhaité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6bfe25b83aa6c1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import os\n",
    "from ray.train import Checkpoint\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from ray.air import session\n",
    "import tempfile\n",
    "\n",
    "\n",
    "# Train the model\n",
    "def train_genre(config, MAX_EPOCH=1000, SHOW_BAR=True):\n",
    "    # Initialisation\n",
    "    running_losses = []\n",
    "    validation_losses = []\n",
    "    model = config['model'](n1=config['n1'], n2=config['n2'], n3=config['n3'], n4=config['n4'],\n",
    "                            p1=config['p1'], p2=config['p2'], p3=config['p3'], p4=config['p4']).to(device)\n",
    "\n",
    "    loss_fn = config['loss_fn']()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay=1e-5)\n",
    "\n",
    "    # We load the training at a specific checkpoint if wanted\n",
    "    checkpoint = session.get_checkpoint()\n",
    "    if checkpoint:\n",
    "        checkpoint_state = checkpoint.to_dict()\n",
    "        start_epoch = checkpoint_state[\"epoch\"]\n",
    "        model.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    # Get the DataLoaders\n",
    "    train_batch, validation_batch = load_data_genre(batch_size=config['batch_size'])\n",
    "\n",
    "    # Used for a better display using tqdm\n",
    "    if SHOW_BAR:\n",
    "        bar = tqdm(total=MAX_EPOCH)\n",
    "        bar.update(start_epoch)\n",
    "\n",
    "    # Training for every epoch\n",
    "    for epoch in range(start_epoch, MAX_EPOCH):\n",
    "        running_loss = 0.\n",
    "        model.train()\n",
    "\n",
    "        # Computes the running loss during training\n",
    "        for _, data in enumerate(train_batch):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        validation_loss = 0.\n",
    "        accuracy = 0\n",
    "        validation_total = 0\n",
    "        model.eval()\n",
    "\n",
    "        # Test the model and evaluates the performances\n",
    "        for _, data in enumerate(validation_batch):\n",
    "            # No computation of the gradient for better performances\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                outputs = model(inputs).squeeze()\n",
    "                loss = loss_fn(outputs, labels.squeeze())\n",
    "                validation_loss += loss.item()\n",
    "                validation_total += labels.size(0)\n",
    "                accuracy += (torch.argmax(outputs, 1) == labels).sum().item()\n",
    "\n",
    "        running_loss /= len(train_batch)\n",
    "        validation_loss /= len(validation_batch)\n",
    "        running_losses.append(running_loss)\n",
    "        validation_losses.append(validation_loss)\n",
    "\n",
    "        # Creates a checkpoint if necessary\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            path = os.path.join(temp_checkpoint_dir, \"checkpoint.pt\")\n",
    "            torch.save(\n",
    "                (model.state_dict(), optimizer.state_dict()), path\n",
    "            )\n",
    "            checkpoint = Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "            ray.train.report(\n",
    "                {\"loss\": validation_loss, \"accuracy\": accuracy / validation_total},\n",
    "                checkpoint=checkpoint,\n",
    "            )\n",
    "\n",
    "        # Handles a proper display\n",
    "        if SHOW_BAR:\n",
    "            bar.update(1)\n",
    "            bar.set_postfix(str=f\"Running loss: {running_loss:.5f} - Validation loss: {validation_loss:.5f}.\")\n",
    "    if SHOW_BAR:\n",
    "        bar.close()\n",
    "\n",
    "    return running_losses, validation_losses, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0fe9df",
   "metadata": {},
   "source": [
    "Il est désormais temps d'utiliser tout ce que nous avons fait précédemment et d'entraîner nos différents modèles en faisant varier les paramètres afin de trouver la meilleure configuration possible. Pour faire cela nous utilisons `ray[tune]`, une librairie Python permettant notamment de faire varier divers paramètres, le tout en lançant les processus en parallèles. \n",
    "\n",
    "Par exemple, dans l'objet config, `ray[tune]` nous permet d'essayer toutes les combinaisons possibles de paramètres pour `n1, n2, n3, n4` allant de 1 à 4096, ainsi que pour tous les autres paramètres, comme notamment le `batch_size` et bien évidemment le modèle choisi parmi les deux que nous avons créé. Nous utilisons un ***scheduler*** de type ***ASHA*** (Async Successive Halving) permettant de stopper les configurations non-performantes le plus tôt possible afin de tester un maximum de combinaisons rapidement, contrairement à une méthode *brute-force* classique. Cela représente une énorme quantité de possibilités et il est donc très intéressant de pouvoir tester autant de combinaisons simultanément, bien que cela prenne beaucoup de temps à s'exécuter, même en possédant un bon ordinateur. \n",
    "\n",
    "Nous comparons donc tous les modèles en se basant sur le critère d'*accuracy* et nous affichons la configuration utilisée pour obtenir les meilleurs résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ecb27beea9be3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ray.tune import JupyterNotebookReporter\n",
    "from functools import partial\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray import tune, train\n",
    "\n",
    "# Test a lot of combination of parameters for the NN using tuning\n",
    "config = {\n",
    "    \"n1\": tune.choice(np.arange(1, 4096, 1)),\n",
    "    \"n2\": tune.choice(np.arange(1, 4096, 1)),\n",
    "    \"n3\": tune.choice(np.arange(1, 4096, 1)),\n",
    "    \"n4\": tune.choice(np.arange(1, 4096, 1)),\n",
    "    \"n5\": tune.choice(np.arange(1, 1000, 1)),\n",
    "    \"p1\": tune.choice(np.linspace(0, 1, 100)),\n",
    "    \"p2\": tune.choice(np.linspace(0, 1, 100)),\n",
    "    \"p3\": tune.choice(np.linspace(0, 1, 100)),\n",
    "    \"p4\": tune.choice(np.linspace(0, 1, 100)),\n",
    "    \"p5\": tune.choice(np.linspace(0, 1, 50)),\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-2),\n",
    "    \"batch_size\": tune.choice([256, 512, 1024, 2048]),\n",
    "    'model': tune.choice([NeuralNetwork, MusicGenreClassifier]),\n",
    "    'loss_fn': torch.nn.CrossEntropyLoss,\n",
    "    'loader': load_data_genre\n",
    "}\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=200,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2,\n",
    ")\n",
    "\n",
    "# Handles the results\n",
    "result = tune.run(\n",
    "    partial(train_genre, SHOW_BAR=False),\n",
    "    resources_per_trial={\"cpu\": 8, \"gpu\": 1},\n",
    "    config=config,\n",
    "    num_samples=2,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=JupyterNotebookReporter()\n",
    ")\n",
    "\n",
    "# Find the best config on the accuracy parameter\n",
    "best_trial = result.get_best_trial(\"accuracy\", \"max\", \"last\")\n",
    "print(f\"Best trial config: {best_trial.config}\")\n",
    "print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421d113a",
   "metadata": {},
   "source": [
    "Maintenant que nous avons trouvé une configuration satisfaisante, nous entraînons le modèle avec la meilleure configuration puis on récupère les courbes de pertes que nous traçons sur le graphe ci-dessous. Nous utilisons les courbes afin de détecter que nous ne sommes pas dans un cas d'*under-fitting* ou d'*over-fitting* et nous relançons l'entraînement en modifiant le paramètre *MAX_EPOCH* en fonction du cas dans lequel nous nous trouvons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3ac2e56fb2fe8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# config = {'n1': 2565, 'n2': 4046, 'n3': 1828, 'n4': 2648, 'p1': 0.5757575757575758, 'p2': 0.7676767676767677,\n",
    "#           'p3': 0.686868686868687, 'p4': 0.08080808080808081, 'lr': 0.00024444515305828874, 'batch_size': 1024,\n",
    "#           'model': NeuralNetwork, 'loss_fn': torch.nn.CrossEntropyLoss}\n",
    "\n",
    "# Uses the best config found previously and display the curves of losses\n",
    "r_l, v_l, model = train_genre(best_trial.config, MAX_EPOCH=50)\n",
    "plt.plot(r_l, color='blue')\n",
    "plt.plot(v_l, color='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aa34f8",
   "metadata": {},
   "source": [
    "Nous regardons maintenant l'*accuracy* de prédiction de notre modèle que nous venons d'entraîner. Nous obtenons un score autour de 0.45, légèrement plus faible que pour le ***RandomForest*** qui reste plus performant, et qui de plus prends beaucoup moins de temps à s'entraîner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116b9d1e91a97972",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "\n",
    "# Predict the genre using the NN and print the accuracy of the model\n",
    "train_batch, validation_batch = load_data_genre(batch_size=best_trial.config['batch_size'])\n",
    "\n",
    "y_preds = []\n",
    "y_true = []\n",
    "\n",
    "for i, data in enumerate(validation_batch):\n",
    "    inputs, labels = data\n",
    "    outputs = torch.argmax(model(inputs).squeeze(), 1).cpu()\n",
    "    y_preds.extend(outputs)\n",
    "    y_true.extend(labels.cpu())\n",
    "\n",
    "print(f1_score(y_true, y_preds, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375d10973f1f0760",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Boosting\n",
    "\n",
    "Pour ce troisième modèle, nous décidons d'essayer `xgboost`, une autre librairie très populaire dans le monde du *machine learning* en Python, permettant de faire une methode de type *boosting* sur des arbres. De plus, l'implémentation se fait relativement facilement car elle reste assez similaire à celle utilisée précédemment avec `pytorch`.\n",
    "\n",
    "Nous créons donc la méthode permettant d'entraîner un modèle selon une configuration donnée, en utilisant cette fois des données compatibles avec `xgboost` qui utilise les ***DMatrix***, une classe parfaitement optimisée pour la rapidité et l'utilisation de la mémoire. La syntaxe est relativement explicite, nous séparons toujours notre jeu de données en données d'entraînement et de test, puis nous utilisons la méthode `train` permettant d'entraîner notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bcf6a46a575cce",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from ray import tune, train\n",
    "\n",
    "\n",
    "# Training method\n",
    "def train_boost_genre(config):\n",
    "    # Split the data\n",
    "    X = dataset_train.iloc[:, dataset_train.columns != 'genre'].to_numpy()\n",
    "    y = dataset_train.iloc[:, dataset_train.columns == 'genre'].to_numpy().flatten().astype(np.int8)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=12345)\n",
    "\n",
    "    # Creates the XGBoost model\n",
    "    train_set = xgboost.DMatrix(X_train, label=y_train)\n",
    "    test_set = xgboost.DMatrix(X_test, label=y_test)\n",
    "    results = {}\n",
    "\n",
    "    booster = xgboost.train(\n",
    "        config['params'],\n",
    "        train_set,\n",
    "        evals=[(test_set, \"eval\")],\n",
    "        evals_result=results,\n",
    "        verbose_eval=False,\n",
    "        early_stopping_rounds=config['early_stopping_rounds'],\n",
    "        xgb_model=config['model']\n",
    "    )\n",
    "\n",
    "    # Predict the genre and display the accuracy\n",
    "    y_pred = booster.predict(test_set)\n",
    "    accuracy = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    train.report({\"accuracy\": accuracy, \"done\": True, \"merror\": results['eval']['merror'][-1],\n",
    "                  \"mlogloss\": results['eval']['mlogloss'][-1], \"model\": booster})\n",
    "\n",
    "    return booster, accuracy, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a114bd30",
   "metadata": {},
   "source": [
    "En s'aidant à nouveau de `ray[tune]`, nous faisons varier les paramètres et testons ainsi le plus de combinaisons possibles de modèles en ne gardant que la meilleure. Cette fois encore, la comparaison se fait au niveau du paramètre *accuracy*, nous permettant de garder la configuration qui a eu les meilleures performances lors de la prédiction des données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e975f3443f88147a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "# Tries a lot of configuration by tuning the parameters\n",
    "b = None\n",
    "config = {\n",
    "    'params': {\n",
    "        \"objective\": \"multi:softmax\",\n",
    "        \"eval_metric\": [\"merror\", \"mlogloss\"],\n",
    "        \"max_depth\": tune.randint(1, 200),\n",
    "        \"min_child_weight\": tune.uniform(0, 20),\n",
    "        \"subsample\": tune.uniform(0.2, 1.0),\n",
    "        \"eta\": tune.loguniform(1e-7, 1e-1),\n",
    "        \"num_class\": len(GENRE_MAP),\n",
    "        \"tree_method\": tune.choice(['auto', 'exact', 'approx', 'hist'])\n",
    "    },\n",
    "    \"early_stopping_rounds\": tune.randint(1, 10000),\n",
    "    \"model\": b\n",
    "}\n",
    "scheduler = ASHAScheduler(\n",
    "    max_t=50, grace_period=5, reduction_factor=2\n",
    ")\n",
    "tuner = tune.Tuner(\n",
    "    train_boost_genre,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"mlogloss\",\n",
    "        mode=\"min\",\n",
    "        num_samples=20,\n",
    "        scheduler=scheduler\n",
    "    ),\n",
    "    param_space=config,\n",
    ")\n",
    "\n",
    "# Find the best config on the accuracy parameter\n",
    "results = tuner.fit()\n",
    "best_trial = results.get_best_result(\"accuracy\", \"max\", \"last\")\n",
    "print(f\"Best trial config: {best_trial.config}\")\n",
    "print(f\"Best trial final validation accuracy: {best_trial.metrics['accuracy']:.4f}\")\n",
    "print(f\"Best trial final validation error: {best_trial.metrics['merror']:.4f}\")\n",
    "print(f\"Best trial final validation logloss: {best_trial.metrics['mlogloss']:.4f}\")\n",
    "print(best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e8ca25",
   "metadata": {},
   "source": [
    "Après avoir obtenu la meilleure configuration trouvée, on poursuit l'entraînement de ce modèle en essayant de ne pas faire d'*over-fitting*. Arpès quelques tests, nous avons trouvé que faire 16 ré-entraînements nous offrait les meilleurs résultats.\n",
    "\n",
    "On en profite pour récupérer les valeurs de l'*accuracy*, l'erreur et la perte logarithmique que nous traçons sur les graphes ci-dessous, nous peremetant de trouver une *accuracy* à 0.4496, la meilleure que nous avons trouvé !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f537b9df14cb91",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "b = best_trial.metrics['model'] if best_trial is not None else None\n",
    "merrs = []\n",
    "mloglosss = []\n",
    "accs = []\n",
    "\n",
    "# Train a few times the same model and save the error, log loss and accuracy\n",
    "for _ in trange(16):\n",
    "    b, acc, res = train_boost_genre({\n",
    "        'params': best_trial.config['params'],\n",
    "        \"early_stopping_rounds\": best_trial.config['early_stopping_rounds'],\n",
    "        'model': b\n",
    "    })\n",
    "    merrs.extend(res['eval']['merror'])\n",
    "    mloglosss.extend(res['eval']['mlogloss'])\n",
    "    accs.extend([acc] * len(res['eval']['mlogloss']))\n",
    "\n",
    "# Plot the 3 curves and find the best accuracy\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "fig.add_subplot(3, 1, 1)\n",
    "plt.plot(merrs)\n",
    "plt.title('$M_{err}$')\n",
    "fig.add_subplot(3, 1, 2)\n",
    "plt.plot(mloglosss)\n",
    "plt.title('$M_{logloss}$')\n",
    "fig.add_subplot(3, 1, 3)\n",
    "plt.plot(accs)\n",
    "plt.title('$accuracy$')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best accuracy: {max(accs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff73040",
   "metadata": {},
   "source": [
    "Pour terminer, nous utilisons ce modèle qui semble être le plus satisfaisant parmis les trois essayés et nous faisons comme précédemment: on prédit les genre des données contenues dans `dataset_test`, puis on met à jour le fichier *.csv* que nous pouvons désormais mettre en ligne sur MylearningSpace !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc31aa6399447a7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from xgboost import DMatrix\n",
    "\n",
    "# Predict the genres of the dataset_test and write them in the csv file\n",
    "test_pred = b.predict(DMatrix(dataset_test.iloc[:, dataset_test.columns != 'genre'].to_numpy()))\n",
    "\n",
    "for i in range(len(dataset_test)):\n",
    "    dataset_test.loc[i, 'genre'] = GENRE_MAP[int(test_pred[i])]\n",
    "\n",
    "dataset_test.to_csv('./dataset/spotify_dataset_test_pred.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de5c1d65ef4b11e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercice 2\n",
    "\n",
    "Nous travaillons désormais sur un nouveau set de données, contenues dans le fichier *spotify_dataset_subset.csv*, avec cette fois pour objectif de prédire la popularité d'une musique. En regardant rapidement les données, nous nous rendons compte que les genres sont désormais représentés par des listes de genres, rendant la prédiction beaucoup plus compliquée. On pourrait se dire que les genres n'influent pas sur la popularités au vu du résultat de la matrice des corrélations à l'exercice 1, cependant, après un rapide test sur un modèle de *RandomForest*, il est clair qu'il est nécessaire de les prendre en compte d'une façon ou d'une autre.\n",
    "\n",
    "### Analyse des données\n",
    "\n",
    "Nous parcourons donc le dataset afin d'en remplacer certaines caractéristiques: de la même façon qu'à l'exercice 1, nous remplaçons les *release_dates* par un *timestamp*. Également, nous rajoutons une colonne contenant l'id de l'artiste en encodant le nom de l'artiste afin d'obtenir un entier. Cela nous permettra d'éliminer la colonne *artiste_name* puisqu'elle sera représentée par la colonne *id*.\n",
    "\n",
    "Enfin, pour les genres, nous transformons la colonne *genres* par plusieurs nouvelles colonnes au format *genre_nom*, valant 1 si la musique est du genre donné, et 0 sinon. Pour cela nous utilisons `ast.literal_eval`, permettant de récupérer la liste contenue à l'endroit du dataset, puis on définit sa valeur à 1, créant ainsi une colonne si la colonne n'existe pas encore, ou vient remplir la colonne si elle existe. Il suffit ensuite simplement de remplacer les valeurs à `nan` par des 0 afin que les colonnes représentent des booléens. On remarque en sortie que le programme nous compte 1766 colonnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c3efca610530ef",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import datetime\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas\n",
    "import hashlib\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pandas.errors.PerformanceWarning)\n",
    "\n",
    "dataset = pandas.read_csv(\"dataset/spotify_dataset_subset.csv\")\n",
    "\n",
    "# Transform the explicit boolan in 0 and 1\n",
    "dataset['explicit'] = dataset['explicit'].astype(np.int8)\n",
    "\n",
    "for i, date in enumerate(dataset['release_date']):\n",
    "    # Detect the date format and replace it with a utc timestamp for the train dataset\n",
    "    if len(date.split('-')) == 3:\n",
    "        dataset.loc[i, 'release_date'] = datetime.datetime.strptime(date, \"%Y-%m-%d\").replace(\n",
    "            tzinfo=datetime.timezone.utc).timestamp()\n",
    "    elif len(date.split('-')) == 2:\n",
    "        dataset.loc[i, 'release_date'] = datetime.datetime.strptime(date, \"%Y-%m\").replace(\n",
    "            tzinfo=datetime.timezone.utc).timestamp()\n",
    "    elif len(date.split('-')) == 1:\n",
    "        dataset.loc[i, 'release_date'] = datetime.datetime.strptime(date, \"%Y\").replace(\n",
    "            tzinfo=datetime.timezone.utc).timestamp()\n",
    "    else:\n",
    "        print(date)\n",
    "\n",
    "    # Encode the artist_name into the artist_id\n",
    "    dataset.loc[i, 'artist_id'] = int.from_bytes(hashlib.sha256(dataset.loc[i, 'artist_name'].encode()).digest(), 'big')\n",
    "\n",
    "    # Creates a sub column with the genre and set the value to 1\n",
    "    genres = ast.literal_eval(dataset.loc[i, 'genres'])\n",
    "    for genre in genres:\n",
    "        dataset.loc[i, f\"genre_{genre.lower().replace(' ', '_')}\"] = 1\n",
    "\n",
    "# Replaces all the nan values by 0\n",
    "dataset.replace(np.nan, 0, inplace=True)\n",
    "\n",
    "print(f\"Using {len(dataset.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0d81c3",
   "metadata": {},
   "source": [
    "Maintenant que les données sont dans un format plus facile à traiter, nous pouvons tenter de visualiser les données à l'aide d'un *PCA*. Pour cela, on sépare nos données comme les fois précédentes, en retirant pour *X* les colonnes `genres, id, track_name, artist_name, popularity`. La raison étant, `popularity` est la colonne à prédire, la colonne `genres` n'est plus intéressante puisque nous avons créé une colonne par genre au lieu d'avoir une colonne contenant tous les genres d'une chanson donnée, la colonne `artiste_name` est désormais représentée par `artist_id`, et enfin nous avons supposé que le nom d'une chanson ainsi que son id n'etait pas intéressant étant donné que ce sont (en principe) des données uniques pour chaque chanson (à l'exception de certains titres de chanson mais cela reste très rare).\n",
    "\n",
    "On crée d'abord notre *PCA*, puis on affiche les graphes de la variance et de la variance cumulée afin de déterminer certaines caractéristiques de nos données. On remarque que pour garder 80% de l'information il faut garder autour de 800 composantes, ce qui prouve l'utilité et l'importance de la séparation des genres en colonnes distinctes.\n",
    "\n",
    "Enfin, on utilise cette information en créant notre *PCA* avec 800 composantes, puis on affiche les données en 3D en ne regardant que les 3 premières composantes, qui sont (par construction du *PCA*) les plus représentatives des données étudiées. On remarque une forme assez singulière pour nos données, qui prends une forme d'étoile. Pour autant, il est difficile de faire des conclusions ou suppositions au vu de cet affichage étant donné que chaque branche contient une grande variété de donnée au vu des couleurs des points affichés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77cb37bf56ee062",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creation of the PCA\n",
    "pca = PCA()\n",
    "X = dataset[dataset.columns.difference(['genres', 'id', 'track_name', 'artist_name', 'popularity'])].to_numpy()\n",
    "Y = dataset.iloc[:, dataset.columns == 'popularity'].to_numpy().flatten().astype(np.int8)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "pca.fit(X)\n",
    "var_ratios = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Display of variance and cumulated variance\n",
    "fig = plt.figure(figsize=(20, 5))\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.plot(pca.explained_variance_)\n",
    "plt.title('Explained vairance')\n",
    "\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.plot(var_ratios)\n",
    "plt.title('Variance ratios')\n",
    "\n",
    "# 3D display of the pca with 800 components\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = plt.axes(projection='3d')\n",
    "pca = PCA(n_components=800)\n",
    "X_down = pca.fit_transform(X)\n",
    "\n",
    "ax.scatter(X_down[:, 0], X_down[:, 1], X_down[:, 2], c=Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a141fc",
   "metadata": {},
   "source": [
    "Une seconde méthode de visualisation que nous essayons est le *TSNE* (t-distributed stochastic neighbor embedding). Nous construisons notre *TSNE* de la même façon que le *PCA*, cette fois en ne gardant que 3 composantes afin de faire un affichage en 3 dimensions (La construction du TSNE nous impose *n_components* <= 4 donc on ne peut pas faire avec 800 composantes comme pour le *PCA*).\n",
    "\n",
    "Cette fois encore, l'affichage nous montre un gros bloc de points de toutes les couleurs, confirmant l'idée que les classes ne sont pas facile à distinguer. Cette représentation nous confirme notamment que nous ne pouvons pas vraiment utiliser de méthodes comme le clustering ou la régression car il sera trop difficile de séparer les classes de façon satisfaisante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b92ebac2b0228b5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Same way of seperating the data\n",
    "X = dataset[dataset.columns.difference(['genres', 'id', 'track_name', 'artist_name', 'popularity'])].to_numpy()\n",
    "Y = dataset.iloc[:, dataset.columns == 'popularity'].to_numpy().flatten().astype(np.int8)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Creation of the TSNE with 3 components\n",
    "tnse = TSNE(n_components=3)\n",
    "\n",
    "# 3D display of the points\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = plt.axes(projection='3d')\n",
    "X_down = tnse.fit_transform(X)\n",
    "print(tnse.kl_divergence_)\n",
    "\n",
    "ax.scatter(X_down[:, 0], X_down[:, 1], X_down[:, 2], c=Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52316964",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "La prochaine étape est maintenant l'entraînement d'un modèle afin de faire de la prédiction de la popularité d'une musique. Étant donné que le modèle est relativement similaire, nous essayons les modèles que nous avions utilisé à l'exercice 1, appliqué à l'exercice 2.\n",
    "\n",
    "Nous avons commencé par les réseaux de neuronnes, cependant, les données étaient trop volumineuses pour `ray[tune]` étant donné qu'il y a beaucoup de colonnes dû à la grande quantité de genres. Il était donc impossible de faire du *fine tuning* sur les paramètres et il était donc difficile d'entraîner nos réseaux de neuronnes autrement. Nous avons essayé quelques configurations manuelles malgré tout, mais les résultats n'ont pas été concluant.\n",
    "\n",
    "#### Boosting\n",
    "\n",
    "Nous décidons alors de nous centrer sur le meilleur modèle que nous avions à l'exercice 1: le *boosting* avec `xgboost`. Il ne nous semblait pas pertinent de refaire des *RandomForest* non plus étant donné que `xgboost` utilise des arbres directement. C'est donc ce que nous faisons dans le programme ci-dessous, qui implémente le boosting d'une façon tout à fait similaire à précedemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21d4f19046d347e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def train_boost_popularity(config, model):\n",
    "    # Spliting the data\n",
    "    X = dataset[\n",
    "        dataset.columns.difference(['genres', 'id', 'track_name', 'artist_name', 'popularity', 'artist_id'])].to_numpy()\n",
    "    y = dataset.iloc[:, dataset.columns == 'popularity'].to_numpy().flatten()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=12345)\n",
    "\n",
    "    # Creation of the DMatrix\n",
    "    train_set = xgboost.DMatrix(X_train, label=y_train)\n",
    "    test_set = xgboost.DMatrix(X_test, label=y_test)\n",
    "    results = {}\n",
    "\n",
    "    # Training the model\n",
    "    booster = xgboost.train(\n",
    "        params=config,\n",
    "        dtrain=train_set,\n",
    "        evals=[(test_set, \"eval\")],\n",
    "        evals_result=results,\n",
    "        verbose_eval=False,\n",
    "        early_stopping_rounds=10,\n",
    "        xgb_model=model\n",
    "    )\n",
    "\n",
    "    return booster, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c455e880",
   "metadata": {},
   "source": [
    "Par le même principe qu'à l'exercice précédent, nous entraînons notre modèle une certaine quantité de fois en essayant d'éviter l'*over-fitting*, cette fois-ci en se basant sur le critère de la *RMSE* (Root Mean Square Error), étant donné qu'il serait irréaliste de chercher à prédire la popularité au chiffre près. \n",
    "\n",
    "Après ré-entraînement du modèle, on trace la courbe de la *RMSE* et on remarque qu'elle tourne autour de 24, en descendant à un minimum autour de 22,7, ce qui semble assez satisfaisant au vu du problème."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a95bdff6415f7a3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Booster\n",
    "b = None\n",
    "\n",
    "config = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": [\"rmse\"],\n",
    "    \"max_depth\": 100,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"subsample\": 0.2,\n",
    "    \"eta\": 1e-1,\n",
    "    \"tree_method\": 'auto'\n",
    "}\n",
    "rmses = []\n",
    "\n",
    "# Train the model a few times and saves the rmse\n",
    "for _ in trange(4):\n",
    "    b, res = train_boost_popularity(config, b)\n",
    "    rmses.extend(res['eval']['rmse'])\n",
    "\n",
    "# Plot the curve of the RMSE\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "fig.add_subplot(3, 1, 1)\n",
    "plt.plot(rmses)\n",
    "plt.title('$RMSE$')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a60715",
   "metadata": {},
   "source": [
    "Maintenant que nous avons un booster entraîné avec de bonnes performances, il est temps de le mettre à l'épreuve avec nos données de test en prédisant la popularité de nos données. De là, nous avons décidé de nous référer à deux types de scores: la *MAE* (Mean Absolute Error) ainsi qu'à un score \"créé\" pour notre cas de figure.\n",
    "\n",
    "Ce score en question fonctionne de la façon suivante: si la popularité prédite est comprise autour de plus ou moins 10 de la popularité réelle, alors nous avons prédit correctement, sinon nous avons mal prédit. Cela nous permet donc ensuite de faire un pourcentage des données correctement prédites ou non.\n",
    "\n",
    "Les résultats sont les suivants: une *MAE* autour de 11.5 et environ 54% de données prédites de façon satisfaisantes. Une fois encore, nous considérons les résultats comme globalement satisfaisants en comparaison aux performances obtenues à l'exercice 1 et à la complexité des données à prédire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bccf2f58ad4847",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prediction using the booster\n",
    "y_true = dataset.iloc[:, dataset.columns == 'popularity'].to_numpy().flatten()\n",
    "y_pred = b.predict(xgboost.DMatrix(dataset[dataset.columns.difference(\n",
    "    ['genres', 'id', 'track_name', 'artist_name', 'popularity', 'artist_id'])].to_numpy()))\n",
    "\n",
    "me = 0\n",
    "score = 0\n",
    "\n",
    "# Computation of the scores\n",
    "for i in range(len(y_true)):\n",
    "    diff = abs(y_pred[i] - y_true[i])\n",
    "    me += diff\n",
    "    if diff < 10:\n",
    "        score += 1\n",
    "\n",
    "print(f\"MAE: {me / len(y_true):.4f}\")\n",
    "print(f\"Score: {score / len(y_true) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368a6415",
   "metadata": {},
   "source": [
    "#### Support Vector Regression\n",
    "\n",
    "Pour terminer, nous décidons d'essayer un dernier type de classification: les *SVM* (Support Vector Machines) et plus particulièrement ici les *SVR* (Support Vector Regression). Le programme est relativement simple et très similaire à d'autres processus expliqué précedemment: nous séparons les données puis on entraîne notre modèle avec. Enfin, nous prédisons nos données de test, et nous calculons la *RMSE* entre les données réelles et les données prédites.\n",
    "\n",
    "La réponse est assez claire: presque 680 de *RMSE*, nous pouvons aisément conclure que ce modèle n'est pas du tout adapté à notre situation et qu'il est bien moins performant que le *boosting* une fois de plus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9712741ef3684b32",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import svm\n",
    "\n",
    "svr = svm.SVR()\n",
    "\n",
    "# Spliting the data\n",
    "X = dataset[dataset.columns.difference(['genres', 'id', 'track_name', 'artist_name', 'popularity'])].to_numpy()\n",
    "Y = dataset.loc[:, dataset.columns == 'popularity'].to_numpy().flatten().astype(np.float64)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=12345)\n",
    "\n",
    "# Train the svr and computes the RMSE using the prediction\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "print(mean_squared_error(y_test, svr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780810341f058b00",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercice 3\n",
    "\n",
    "Maintenant que nous savons classer des données de façon relativement satisfaisante, nous pouvons essayer de faire des recommandations aux utilisateurs de la plateforme. Pour cela, il nous faut détecter si deux musiques sont \"proches\", afin de savoir ou non si elle est susceptible de plaire à l'utilisateur.\n",
    "\n",
    "### Formatage des données\n",
    "\n",
    "Comme les fois précédentes, la première étape est de remettre les données sous un format qu'il est possible de traiter pour notre apprentissage. Nous remplaçons donc les `release_dates` par un `timestamp` comme vu précedemment, et également nous retirons les duplications afin de limiter légèrement la quantité de données à traiter.\n",
    "\n",
    "Le seul problème que nous rencontrons lors de cette étape est que nous sommes contraints à ne pas pouvoir prendre en compte la colonne `artists` en l'état. La raison étant que, de la même façon que pour les `genres` à l'exercice 2, les artistes sont désormais représentés sous forme de liste d'artistes. Nous avons donc essayé de faire la même méthode que précédemment en créant une colonne par artiste, mais cela a résulté en un dataset contenant plus de 32 000 colonnes, ce qui était bien trop gros pour l'entraînement du modèle. \n",
    "\n",
    "Pour pallier à ce problème, nous avons essayé de simplifier la colonne des artistes en ne gardant que le premier artiste de la liste, qui souvent est l'artiste principal de la musique en question. Cette technique fonctione très bien, cependant, l'algorithme de détection des musiques proches va détecter presque systématiquement que les musiques les plus proches sont celles écrites par le même artiste, créant ainsi des playlists entières de musiques d'un seul et même artiste. Bien que cela puisse avoir un intérêt, nous avons trouvé qu'il était plus intéressant d'avoir une certaine diversité au sein des recommandations, donc nous avons décidé de simplement retirer la colonne des artistes dans notre entraînement afin de baser les recommandations sur des critères plus variés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c9914d9d72456e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pandas.errors.PerformanceWarning)\n",
    "\n",
    "dataset = pandas.read_csv(\"dataset/recommendation_spotify.csv\")\n",
    "\n",
    "dataset['explicit'] = dataset['explicit'].astype(np.int8)\n",
    "\n",
    "for i in range(dataset.shape[0]):\n",
    "    # Detect the date format and replace it with a utc timestamp for the train dataset\n",
    "    date = dataset.iloc[i]['release_date']\n",
    "    if len(date.split('-')) == 3:\n",
    "        dataset.loc[i, 'release_date'] = datetime.datetime.strptime(date, \"%Y-%m-%d\").replace(\n",
    "            tzinfo=datetime.timezone.utc).timestamp()\n",
    "    elif len(date.split('-')) == 2:\n",
    "        dataset.loc[i, 'release_date'] = datetime.datetime.strptime(date, \"%Y-%m\").replace(\n",
    "            tzinfo=datetime.timezone.utc).timestamp()\n",
    "    elif len(date.split('-')) == 1:\n",
    "        dataset.loc[i, 'release_date'] = datetime.datetime.strptime(date, \"%Y\").replace(\n",
    "            tzinfo=datetime.timezone.utc).timestamp()\n",
    "    else:\n",
    "        print(date)\n",
    "\n",
    "# Remove the duplicates in the dataset\n",
    "dataset.drop_duplicates(inplace=True, subset=['id'])\n",
    "print(f\"Using {len(dataset.columns)} columns and {len(dataset)} lines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdd9a60",
   "metadata": {},
   "source": [
    "### Détection des voisins\n",
    "\n",
    "Nous allons utiliser la méthode des ***Nearest Neighbors*** afin de trouver les musiques les plus similaires à une musique donnée en fonction de ses particularités. Afin de construire une playlist d'une dizaine de musiques, il suffit alors de trouver les 10 plus proches voisins afin de proposer une playlist la plus pertinente possible (méthode des *KNN*).\n",
    "\n",
    "Nous décidons ici de faire un premier calcul des plus proches voisins en renseignant donc 11 voisins (un point est toujours voisin de lui-même) et en renseignant le mode sur `auto` afin que l'algorithme trouve le meilleur paramètre à choisir directement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91825255d7785d70",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Extract the data and find the nearest 10 neighbors for every song\n",
    "X = dataset[dataset.columns.difference(['artists', 'id', 'name'])].to_numpy()\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=11, algorithm='auto').fit(X)\n",
    "distances, indices = nbrs.kneighbors(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4cfecf",
   "metadata": {},
   "source": [
    "Ensuite nous créons deux méthodes qui vont nous aider pour faire les playlists: une première méthode que l'on nomme `make_playlist`, qui prend en paramètre l'id d'une musique et le nombre de voisins à trouver, puis qui va effectuer l'algorithme des *KNN* pour cette musique.\n",
    "\n",
    "La deuxième méthode `make_playlist_user` est légèrement plus complexe, on récupère une liste de musiques, et on va renvoyer une playlist en fonction de cette liste de musiques. Pour faire cela, on attribue un poids à chaque musique de la liste, et on va créer des petites playlists autour de chaque musique de la liste en fonction de son poids, qui est lui-même calculé en fonction du nombre de fois que la musique a été écoutée. Voici un exemple simpliste: si on donne une liste contenant la musique A et la musique B, que la musique A a été écoutée 10 fois et la musique B 1 fois, on va renvoyer une playlist contenant 10 fois plus de musiques proches de la musique A que de la musique B.\n",
    "\n",
    "Plusieurs choses sont à noter dans le fonctionnement de ce programme cependant: premièrement, dans `make_playlist`, nous utilisons la variable globale indices, définie précédemment. Cette variable contient le résultat des *KNN* dans la cellule précédente, c'est à dire les 10 plus proches voisins de chaque musique. Si l'on remarque que la taille demandé est plus petite que la taille déjà calculée, on ne refait pas tourner l'algorithme afin de gagner en performances. De plus, dans la méthode `make_playlist_user`, comme on arrondi les poids à des nombres entiers, il est possible que la taille de la playlist contenant les musiques soit légèrement plus grande que la taille demandée. Nous aurions pu décider de renvoyer la playlist en l'état, mais nous avons préféré tronqué les musiques trouvées afin de convenir parfaitement à la taille demandé en paramètre, ce qui explique le `song[:size]` à la fin de la méthode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2986190c28a164dd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "def make_playlist(song_index, size=10):\n",
    "    global indices\n",
    "\n",
    "    # We don't run the algorithm again if we already have the data from the previous cell\n",
    "    if size >= indices.shape[1]:\n",
    "        nbrs = NearestNeighbors(n_neighbors=size + 1, algorithm='auto').fit(X)\n",
    "        distances, indices = nbrs.kneighbors(X)\n",
    "\n",
    "    # only return for 1: size+1 because 0 is the song and the shape might be bigger\n",
    "    return indices[song_index][1:size + 1]\n",
    "\n",
    "\n",
    "def make_playlist_for_user(songs_index, songs_weight, size=10):\n",
    "    # the more a song has been played by a user, the most similar songs we will recommand\n",
    "    # Eg: user listened 10 times to song A and 1 time to song B, playlist will be 10 songs like A and 1 song like B\n",
    "\n",
    "    songs_weight_total = np.sum(songs_weight)\n",
    "    songs = []\n",
    "\n",
    "    for i in range(len(songs_index)):\n",
    "        # Calculation of the weight and makes the playlist\n",
    "        n = max(1, round(songs_weight[i] / songs_weight_total * size))\n",
    "        songs.extend(make_playlist(songs_index[i], size=n))\n",
    "\n",
    "    # We only return until [:size] because using the weights, we might have more songs than the size we asked for    \n",
    "    np.random.shuffle(songs)\n",
    "    return songs[:size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf13d5d7",
   "metadata": {},
   "source": [
    "### Création des playlists\n",
    "\n",
    "Il est maintenant temps de répondre à la première question en créant ainsi une playlist d'environ une dizaine de musiques en fonction d'une musique donnée. Pour cela, il nous suffit d'articuler nos différents outils correctement: on choisi aléatoirement une musique, puis on crée une playlist contenant 10 musiques ressemblant à la musique renseignée et on affiche le tout !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9471727374355951",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "# Takes a random song and makes a playlist\n",
    "random_song_i = np.random.randint(X.shape[0])\n",
    "song = dataset.iloc[random_song_i]\n",
    "artists = \", \".join(ast.literal_eval(song['artists']))\n",
    "pl = make_playlist(random_song_i, 10)\n",
    "\n",
    "# Clean display of the playlist\n",
    "print(f\"A playlist for {song['name']} by {artists}:\")\n",
    "for s in pl:\n",
    "    song = dataset.iloc[s]\n",
    "    artists = \", \".join(ast.literal_eval(song['artists']))\n",
    "    print(f\"\\t- {song['name']} by {artists}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f60585",
   "metadata": {},
   "source": [
    "Pour terminer cette analyse de données, nous créons un dernier exemple en simulant une expérience utilisateur. Pour faire cela, on crée un dictionnaire *user* contenant une quantité aléatoire de musiques (arbitrairement en 4 et 40 musiques) que l'utilisateur aura écouté chacune un nombre aléatoire de fois (arbitrairement entre 1 et 10 fois). Une fois cela fait, on utilise notre méthode `make_playlist_for_user` afin de créer une playlist d'une certaine quantité de musiques (ici 100 pour l'exemple), basées sur l'expérience de l'utilisateur sur la plateforme. Enfin, nous affichons la playlist créée par notre algorithme !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d42f527b960b78",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "# Contains data with format song_id: times_listened\n",
    "user = {}\n",
    "\n",
    "# Add a random number of songs, listened a random number of times\n",
    "for _ in range(np.random.randint(4, 40)):\n",
    "    user[np.random.randint(X.shape[0])] = np.random.randint(1, 10)\n",
    "\n",
    "# Creates a playlist of 100 songs for the user\n",
    "playlist = make_playlist_for_user(list(user.keys()), list(user.values()), 100)\n",
    "\n",
    "# Clean display of the playlist created\n",
    "print(user)\n",
    "for s in playlist:\n",
    "    song = dataset.iloc[s]\n",
    "    artists = \", \".join(ast.literal_eval(song['artists']))\n",
    "    print(f\"\\t- {song['name']} by {artists}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840441d0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "En conclusion, cette analyse de données nous a permis de mettre en application beaucoup de méthodes différentes de classification des données, le tout appliqué à un cas réel et commun dans la vie de tous les jours. Il était assez surprenant de voir la difficulté de prédiction des données dû au grand nombre de paramètres et à leur variété. Cela nous a contraint à trouver des astuces pour contrer les difficultés rencontrées afin d'analyser au mieux les données proposées.\n",
    "\n",
    "Plus particulièrement, nous avons constaté que la méthode de *boosting* était celle qui convenait le mieux dans les différents essais que nous avons fait, en plus d'être assez performante en matière de temps d'exécution. Les *RandomForest* sont également très intéressants du fait de leur rapidité d'exécution encore plus performante que le *boosting*, bien que les résultats soient légèrement moins satisfaisants. Les *RandomForest* sont en revanche plus facile à écrire, rendant la tâche moins fastidieuse pour un cas d'étude simple. Enfin, les réseaux de neuronnes que nous avons pu tester ont offerts des résultats assez satisfaisants, mais étaient vraiment coûteux en matière de temps de calcul. Peut-être que nous aurions pû trouver des modèles encore plus performants avec une puissance de calcul plus importante."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
